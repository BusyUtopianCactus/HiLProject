{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to given a folder with documents, return a folder with different csvs of common data from the documents.\n",
    "and \n",
    "Evaluation  \n",
    "\n",
    "*** !!!! This contains code from https://github.com/microsoft/table-transformer\n",
    "the areas where code has been taken and modified are labeled clearly. This includes the \n",
    "factor2dmass method which finds the most similar substructure for evaluation purposes\n",
    "\n",
    "This notebook relies on cloning the repo as running DETR+TATR for evaluation utilizes their inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../table-transformer/')\n",
    "sys.path.append('C:\\\\Users\\\\13109\\\\Documents\\\\college\\\\fall2023\\\\8803\\\\project\\\\table-transformer\\\\src')\n",
    "sys.path.append('C:\\\\Users\\\\13109\\\\Documents\\\\college\\\\fall2023\\\\8803\\\\project\\\\table-transformer\\\\detr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import TableExtractionPipeline,visualize_detected_tables,visualize_cells\n",
    "\n",
    "# Create inference pipeline\n",
    "detection_config_path = 'src/detection_config.json'\n",
    "detection_model_path = \"src/pubtables1m_detection_detr_r18.pth\" #'../pubtables1m_detection_detr_r18.pth'\n",
    "detection_device = 'cpu'\n",
    "structure_config_path = 'src/structure_config.json'\n",
    "structure_model_path = 'TATR-v1.1-Fin-msft.pth'\n",
    "structure_device = 'cpu'\n",
    "pipe = TableExtractionPipeline(det_config_path=detection_config_path, det_model_path=detection_model_path, det_device=detection_device, str_config_path=structure_config_path, str_model_path=structure_model_path, str_device=structure_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_words(page):\n",
    "    words = []\n",
    "\n",
    "    for text_word in page.get_text_words():\n",
    "        word = {'bbox': list(text_word[:4]),\n",
    "                'text': text_word[4],\n",
    "                'block_num': text_word[5],\n",
    "                'line_num': text_word[6],\n",
    "                'span_num': text_word[7],\n",
    "                'flags': 0}\n",
    "        words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recognize tables from document images ## used for runtime measuring as well\n",
    "documents_path = '../documents_1210'\n",
    "for filename in list(os.listdir(documents_path)):\n",
    "    output_dir = f\"../detrtatr_csvs_1210\"\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "    doc = fitz.open(os.path.join(documents_path, filename))\n",
    "    page = doc.load_page(0)\n",
    "    words = get_page_words(page) # or use FinTabNet.c // image table words but scaled to whole page from bounding box coords\n",
    "    pix = page.get_pixmap()\n",
    "    output = f\"{documents_path}/document_images/{filename.replace('.pdf','' )}.jpg\"\n",
    "    pix.save(output)\n",
    "    pix.pil_save(output)\n",
    "    img = Image.open(output)\n",
    "    img_path = output\n",
    "    \n",
    "    print(img_path)\n",
    "    extracted_tables = pipe.extract(img, tokens=words, out_objects=True, out_cells=True,\n",
    "                out_html=False, out_csv=True)\n",
    "    for table_idx, extracted_table in enumerate(extracted_tables):\n",
    "        for key, val in extracted_table.items():\n",
    "            if key == 'csv':\n",
    "                csv_filename = os.path.join(output_dir, filename.replace(\".pdf\", \".csv\"))\n",
    "                if val != None and len(val) != 0 and val[0] != None:\n",
    "                    with open(csv_filename, 'w', encoding = 'utf-8') as f:\n",
    "                        f.write(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$env:PYTHONPATH = \"C:\\Users\\13109\\Documents\\college\\fall2023\\8803\\project\\table-transformer\\src;\" + $env:PYTHONPATH\n",
    "$env:PYTHONPATH = \"C:\\Users\\13109\\Documents\\college\\fall2023\\8803\\project\\table-transformer\\detr;\" + $env:PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only to save csvs for ground truth. table_transformer main code was modified to save during the benchmark evaluation execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/main.py --mode eval --data_type structure --config_file 'src/structure_config.json' --data_root_dir \"../FinTabNet-r.c/FinTabNet.c_Image_Structure_PASCAL_VOC\" --model_load_path TATR-v1.1-Fin-msft.pth --table_words_dir \"../FinTabNet-r.c/FinTabNet.c_Image_Table_Words_JSON\" --test_max_size 50 --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"..\\src\\pubtables1m_detection_detr_r18.pth\"\n",
    "\"..\\src\\detection_config.json\"\n",
    "\"..\\documents_1210\\document_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference pipeline to test token and table extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/inference.py --mode extract --detection_config_path \".\\src\\detection_config.json\" --detection_model_path \".\\src\\pubtables1m_detection_detr_r18.pth\" --detection_device cpu --structure_config_path 'src/structure_config.json' --structure_model_path TATR-v1.1-Fin-msft.pth --structure_device cpu --image_dir \"..\\documents_1210\\document_images\" --words_dir \"..\\\\FinTabNet-r.c\\\\FinTabNet.c_Image_Table_Words_JSON\" --out_dir \"../inference_1210/\" -c --crop_padding 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from src.grits import factored_2dmss, iou\n",
    "import itertools\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "list_of_ground_truth = set()\n",
    "for csvs in os.listdir('../true_csvs/'):\n",
    "    if csvs.endswith('.csv'):\n",
    "        table_name = csvs.split('/')[-1]\n",
    "        table_name = table_name.split('_')[:4]\n",
    "        table_name = \"_\".join(table_name)\n",
    "        list_of_ground_truth.add(table_name)\n",
    "print(len(list_of_ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cam in os.listdir('../camelot_csv/camelot_evaluation_csv'):\n",
    "    #rename without camelot_eval\n",
    "    if cam.endswith('.csv'):\n",
    "        table_name = cam.split('/')[-1]\n",
    "        table_name = table_name.split('_')[:4]\n",
    "        table_name = \"_\".join(table_name)\n",
    "        os.rename(f'../camelot_csv/camelot_evaluation_csv/{cam}', f'../camelot_csv/camelot_evaluation_csv/{table_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = \".//true_csvs\"\n",
    "detrtatr_path = \".//detrtatr_csvs\"\n",
    "camelot_path = \".//camelot_csv/camelot_evaluation_csv\"\n",
    "combined_path = \".//combined_csvs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(flattened):\n",
    "    # remove punctuation\n",
    "    flattened = flattened.replace(',', '')\n",
    "    flattened = flattened.replace('.', '')\n",
    "    # remove long whitespace\n",
    "    flattened = flattened.replace('\\n', ' ')\n",
    "    flattened = flattened.replace('\\t', ' ')\n",
    "    flattened = flattened.replace('   ', ' ')\n",
    "    return flattened \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_2d_outer(shape1, shape2, rewards):\n",
    "    num_rows, num_cols = min(shape1[0], shape2[0]), min(shape1[1], shape2[1])\n",
    "    return list(range(num_rows)), list(range(num_cols)), 0\n",
    "\n",
    "def compute_fscore(positive_match_score, num_true, num_pos):\n",
    "    precision = positive_match_score / num_true if num_true > 0 else 0\n",
    "    recall = positive_match_score / num_pos if num_pos > 0 else 0\n",
    "    fscore = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return fscore, precision, recall\n",
    "\n",
    "def lcs_similarity(string1, string2):\n",
    "    string1 =str(string1); string2 = str(string2)\n",
    "    if len(string1) == 0 and len(string2) == 0:\n",
    "        return 1\n",
    "    s = SequenceMatcher(None, string1, string2)\n",
    "    lcs = ''.join([string1[block.a:(block.a + block.size)] for block in s.get_matching_blocks()])\n",
    "    return 2*len(lcs)/(len(string1)+len(string2))\n",
    "\n",
    "def factored_2dmss(df1, df2, reward_function):\n",
    "    # Convert DataFrames to NumPy arrays\n",
    "    \"\"\"\n",
    "    based on code from eval.py in \n",
    "    \"\"\"\n",
    "    true_cell_grid = df1.to_numpy()\n",
    "    pred_cell_grid = df2.to_numpy()\n",
    "\n",
    "    pre_computed_rewards = {}\n",
    "    transpose_rewards = {}\n",
    "    for trow, tcol, prow, pcol in itertools.product(range(true_cell_grid.shape[0]),\n",
    "                                                    range(true_cell_grid.shape[1]),\n",
    "                                                    range(pred_cell_grid.shape[0]),\n",
    "                                                    range(pred_cell_grid.shape[1])):\n",
    "\n",
    "        reward = reward_function(true_cell_grid[trow, tcol], pred_cell_grid[prow, pcol])\n",
    "\n",
    "        pre_computed_rewards[(trow, tcol, prow, pcol)] = reward \n",
    "        transpose_rewards[(tcol, trow, pcol, prow)] = reward\n",
    "\n",
    "    num_pos = pred_cell_grid.shape[0] * pred_cell_grid.shape[1]\n",
    "    num_true = true_cell_grid.shape[0] * true_cell_grid.shape[1]\n",
    "\n",
    "    # Align rows and columns\n",
    "    true_row_nums, pred_row_nums, row_pos_match_score = align_2d_outer(true_cell_grid.shape[:2],\n",
    "                                                                       pred_cell_grid.shape[:2],\n",
    "                                                                       pre_computed_rewards)\n",
    "    true_column_nums, pred_column_nums, col_pos_match_score = align_2d_outer(true_cell_grid.shape[:2][::-1],\n",
    "                                                                             pred_cell_grid.shape[:2][::-1],\n",
    "                                                                             transpose_rewards)\n",
    "\n",
    "    pos_match_score_upper_bound =  min(row_pos_match_score, col_pos_match_score)\n",
    "    upper_bound_score, _, _ = compute_fscore(pos_match_score_upper_bound, num_pos, num_true)\n",
    "\n",
    "    # Calculate the positive match score\n",
    "    positive_match_score = 0\n",
    "    for true_row_num, pred_row_num in zip(true_row_nums, pred_row_nums):\n",
    "        for true_column_num, pred_column_num in zip(true_column_nums, pred_column_nums):\n",
    "            positive_match_score += pre_computed_rewards[(true_row_num, true_column_num, pred_row_num, pred_column_num)]\n",
    "\n",
    "    # Compute the F-score, precision, and recall\n",
    "    fscore, precision, recall = compute_fscore(positive_match_score, num_true, num_pos)\n",
    "    \n",
    "    return fscore, precision, recall, upper_bound_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grits_2dmss(ground_truth_df, model_output_df):\n",
    "    # put to np arrays\n",
    "    ground_truth_df = ground_truth_df.to_numpy()\n",
    "    model_output_df = model_output_df.to_numpy()\n",
    "    # run grits\n",
    "    print(model_output_df.shape, ground_truth_df.shape)\n",
    "\n",
    "    return factored_2dmss(ground_truth_df, model_output_df, iou)\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    # Calculate the Jaccard Similarity between two sets\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union) if len(union) != 0 else 1\n",
    "\n",
    "def shape_accuracy(ground_truth_shape, model_output_shape):\n",
    "    # Calculate the percentage match for rows and columns\n",
    "    row_accuracy = min(ground_truth_shape[0], model_output_shape[0]) / max(ground_truth_shape[0], model_output_shape[0])\n",
    "    col_accuracy = min(ground_truth_shape[1], model_output_shape[1]) / max(ground_truth_shape[1], model_output_shape[1])\n",
    "\n",
    "    # print('row accuracy: ',  str(row_accuracy))\n",
    "    # print('col accuracy: ', str(col_accuracy))\n",
    "    # Calculate overall structural accuracy\n",
    "    return (row_accuracy + col_accuracy) / 2 \n",
    "\n",
    "def compare_contents(ground_truth_df, model_output_df):\n",
    "    # Flatten the DataFrames into lists or sets\n",
    "    ground_truth_content = ' '.join(ground_truth_df.astype(str).values.flatten()) + ' '.join(ground_truth_df.columns)\n",
    "    model_output_content = ' '.join(model_output_df.astype(str).values.flatten()) + ' '.join(model_output_df.columns)\n",
    "    ground_truth_content = clean(ground_truth_content)\n",
    "    model_output_content = clean(model_output_content)\n",
    "\n",
    "    vectorizer = CountVectorizer().fit_transform([ground_truth_content, model_output_content])\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    return cosine_similarity(vectors)[0][1] * 100\n",
    "\n",
    "\n",
    "def compare_tables(ground_truth_csv, model_output_csv):\n",
    "    column_similarity = jaccard_similarity(set(ground_truth_csv.columns), set(model_output_csv.columns))\n",
    "    shape_acc = shape_accuracy(ground_truth_csv.shape, model_output_csv.shape)\n",
    "    content_accuracy = compare_contents(ground_truth_csv, model_output_csv)\n",
    "    jac = jaccard_similarity(set(ground_truth_csv.values.flatten()), set(model_output_csv.values.flatten()))\n",
    "    grits = factored_2dmss(ground_truth_csv, model_output_csv, lcs_similarity)\n",
    "\n",
    "    return shape_acc * 100, content_accuracy,  column_similarity*100, jac, grits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = \".//true_csvs\"\n",
    "detrtatr_path = \".//detrtatr_csvs\"\n",
    "camelot_path = \".//camelot_csv/camelot_evaluation_csv\"\n",
    "combined_path = \".//combined_csvs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = []\n",
    "test_acc = [0,0,0,0, 0]\n",
    "combined_acc = [0,0,0,0, 0]; combined_count = 0\n",
    "camelot_acc = [0,0,0,0, 0]\n",
    "test_grits = []\n",
    "combined_grits = []\n",
    "camelot_grits = []\n",
    "count = 0\n",
    "for doc in os.listdir(combined_path):\n",
    "    doc_name = doc.split('_')[:4]\n",
    "    doc_name = \"_\".join(doc_name)\n",
    "    if doc.endswith('.csv'):\n",
    "            true_csv_ = pd.read_csv(f'../{ground_truth_path}/{doc_name.strip(\".csv\")}_true_table_0.csv')\n",
    "            detrtatr_csvs_ = f'../{detrtatr_path}/{doc_name}'\n",
    "            combined_csvs_ = f'../{combined_csvs_}/{doc_name}'\n",
    "            camelot_csvs_ = f'{camelot_csvs_}/{doc_name}'\n",
    "            for id, table in enumerate([detrtatr_csvs_, combined_csvs_, camelot_csvs_]):\n",
    "                try:\n",
    "                    table_ = pd.read_csv(table)\n",
    "                except:\n",
    "                    print(\"error\", table)\n",
    "                    continue # Track to remove errors in filepaths\n",
    "                print()\n",
    "                print(\"Accuracy:\")\n",
    "                results = (compare_tables(true_csv_, table_))\n",
    "                print(id, results)\n",
    "                if id == 0:\n",
    "                    test_acc = [acc+r for acc, r in zip(test_acc[:-1], results[:-1])]\n",
    "                    test_grits.append(results[-1])\n",
    "                elif id == 1:\n",
    "                    combined_acc = [acc+r for acc, r in zip(combined_acc[:-1], results[:-1])]\n",
    "                    combined_grits.append(results[-1])\n",
    "                else:\n",
    "                    camelot_acc = [acc+r for acc, r in zip(camelot_acc[:-1], results[:-1])]\n",
    "                    camelot_grits.append(results[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detr+Tatr\n",
      "Average F-score :  0.684\n",
      "Average Precision : 0.721\n",
      "Average Recall : 0.651\n",
      "\n",
      "Combined\n",
      "Average F-score : 0.692\n",
      "Average Precision : 0.679 \n",
      "Average Recall : 0.705\n",
      "\n",
      "Camelot\n",
      "Average F-score : 0.766\n",
      "Average Precision : 0.485\n",
      "Average Recall : 0.595\n"
     ]
    }
   ],
   "source": [
    "for i,  grits in enumerate([test_grits, combined_grits, camelot_grits]):\n",
    "    print(['Detr+Tatr','Combined','Camelot'][i])\n",
    "    temp = []\n",
    "    for col in range(4):\n",
    "        temp.append(sum([g[col] for g in grits]))\n",
    "    for label, score in zip(['Average F-score', 'Average Precision', 'Average Recall'], temp):\n",
    "        score = \n",
    "        print(label,' : ', round(score/ len(list_of_ground_truth), 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get randomized csvs from eval + detrtatr run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all csvs in test_csvs_1210, \n",
    "pdfs = \"../table-transformer/fintabnet/fintabnet/pdf/\"\n",
    "save_path = \"../documents\"\n",
    "list_of_ground_truth = set()\n",
    "for csv_ in os.listdir('../test_csvs/'):\n",
    "    if csv_.endswith('.csv'):\n",
    "        csv_name = csv_.split('_')[:4]\n",
    "        csv_name = \"_\".join(csv_name)\n",
    "        list_of_ground_truth.add(csv_name)\n",
    "        #\n",
    "        company, year, quarter, page = csv_name.split('_')\n",
    "        print(company, year, quarter, page)\n",
    "        path = pdfs + company + '/' + year + '/' + quarter + '_' + page + '.pdf'\n",
    "        print(path)\n",
    "        print(csv_name)\n",
    "        doc = fitz.open(path)\n",
    "        # save pdf in save_path\n",
    "        print(save_path +'/' + csv_name + '.pdf')\n",
    "        doc.save(save_path +'/' + csv_name + '.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
